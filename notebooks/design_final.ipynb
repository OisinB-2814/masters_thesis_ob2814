{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "random.seed(2814)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/processed/3_seai_miss_forest_imputation.csv')\n",
    "# Read in our cleaned up SEAI data\n",
    "df = df.drop('BerRating', axis = 1)\n",
    "df = df.drop('CO2Rating', axis = 1)\n",
    "df['NoOfSidesSheltered'] = df['NoOfSidesSheltered'].astype('category')\n",
    "\n",
    "df = df.sample(30000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df[['YearofConstruction', 'GroundFloorArea(sq m)', 'TotalDeliveredEnergy', 'EnergyRating']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/a/52935270/5923619\n",
    "# One Hot Encodes our categorical feature and binds it to the original dataset\n",
    "def encode_and_bind(original_dataframe, feature_to_encode):\n",
    "    dummies = pd.get_dummies(original_dataframe[[feature_to_encode]])\n",
    "    res = pd.concat([original_dataframe, dummies], axis=1)\n",
    "    return(res)\n",
    "\n",
    "\n",
    "# One Hot Encode all of our categorical features\n",
    "df = encode_and_bind(df, 'CountyName')\n",
    "df = encode_and_bind(df, 'DwellingTypeDescr')\n",
    "df = encode_and_bind(df, 'MainSpaceHeatingFuel')\n",
    "df = encode_and_bind(df, 'MainWaterHeatingFuel')\n",
    "df = encode_and_bind(df, 'VentilationMethod')\n",
    "df = encode_and_bind(df, 'StructureType')\n",
    "df = encode_and_bind(df, 'InsulationType')\n",
    "df = encode_and_bind(df, 'NoOfSidesSheltered')\n",
    "\n",
    "# Dropping the unencoded columns for now\n",
    "df = df.drop(['CountyName', 'NoOfSidesSheltered','DwellingTypeDescr', 'MainSpaceHeatingFuel', 'MainWaterHeatingFuel', 'VentilationMethod', 'StructureType','InsulationType'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_cols = [col for col in df.columns if col != 'EnergyRating'] + ['EnergyRating']\n",
    "df = df[new_cols]\n",
    "\n",
    "del(new_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "C2    3766\n",
       "C3    3645\n",
       "D1    3409\n",
       "C1    3332\n",
       "D2    2915\n",
       "B3    2360\n",
       "G     1967\n",
       "E1    1664\n",
       "A3    1503\n",
       "E2    1344\n",
       "F     1307\n",
       "A2    1283\n",
       "B2    1016\n",
       "B1     442\n",
       "A1      47\n",
       "Name: EnergyRating, dtype: int64"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.EnergyRating.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:, :-1] # Independent Variables\n",
    "y = df.iloc[:, -1] # Dependent Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_validate\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=2814)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([4.750597  , 4.83933616, 4.32902718, 4.71308494, 4.41085505]),\n",
       " 'score_time': array([0.16176796, 0.19692087, 0.10724807, 0.21867704, 0.15784287]),\n",
       " 'test_score': array([0.45928571, 0.44428571, 0.46166667, 0.455     , 0.45761905])}"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instead of using scikit learn's pipeline, we import from imblearn\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "# just like we did for StandardScaler, we instantiate SMOTE within the pipeline\n",
    "pipe = Pipeline(steps = [('smote', SMOTE(k_neighbors=17)), \n",
    "                      ('quantilescaler', RobustScaler()),\n",
    "                      ('rfc', RandomForestClassifier(criterion='entropy'))])\n",
    "pipe.fit(X_train, y_train)\n",
    "# cross validation using intra-fold sampling\n",
    "cross_validate(pipe, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "parameters = {\n",
    "    'smote__k_neighbors': [2, 3, 10, 12],\n",
    "    'rfc__n_estimators': [4, 6, 9,], \n",
    "    'rfc__max_features': ['log2', 'sqrt'],\n",
    "    'rfc__criterion': ['entropy', 'gini'], \n",
    "    'rfc__max_depth': [2, 3, 5, 10], \n",
    "    'rfc__min_samples_split': [2, 3, 5],\n",
    "    'rfc__min_samples_leaf': [1, 5, 8] }\n",
    "\n",
    "search = GridSearchCV(pipe, parameters, cv=5)\n",
    "\n",
    "search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# instead of using scikit learn's pipeline, we import from imblearn\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "# just like we did for StandardScaler, we instantiate SMOTE within the pipeline\n",
    "pipe = Pipeline(steps = [('smote', SMOTE(k_neighbors=6)), \n",
    "                      ('quantilescaler', QuantileTransformer()),\n",
    "                      ('rfc', RandomForestClassifier(criterion='entropy', max_depth=10, max_features='sqrt', min_samples_leaf=1, min_samples_split=5, n_estimators=9))])\n",
    "pipe.fit(X_train, y_train)\n",
    "# cross validation using intra-fold sampling\n",
    "cross_validate(pipe, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pipe.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          A1       0.00      0.00      0.00        11\n",
      "          A2       0.77      0.79      0.78       397\n",
      "          A3       0.75      0.71      0.73       455\n",
      "          B1       0.37      0.37      0.37       131\n",
      "          B2       0.38      0.35      0.37       280\n",
      "          B3       0.46      0.44      0.45       702\n",
      "          C1       0.49      0.46      0.47      1056\n",
      "          C2       0.43      0.47      0.45      1109\n",
      "          C3       0.41      0.43      0.42      1080\n",
      "          D1       0.41      0.44      0.43      1031\n",
      "          D2       0.41      0.46      0.44       839\n",
      "          E1       0.31      0.28      0.29       509\n",
      "          E2       0.33      0.25      0.29       408\n",
      "           F       0.41      0.37      0.39       406\n",
      "           G       0.74      0.74      0.74       586\n",
      "\n",
      "    accuracy                           0.47      9000\n",
      "   macro avg       0.44      0.44      0.44      9000\n",
      "weighted avg       0.47      0.47      0.47      9000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred)) #classification report from sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([1.03332806, 1.02375102, 1.03040409, 1.02716804, 1.02296424]),\n",
       " 'score_time': array([0.47879791, 0.47944117, 0.47167206, 0.47490573, 0.47498083]),\n",
       " 'test_score': array([0.68869048, 0.68666667, 0.68838095, 0.68792857, 0.68852381])}"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "# just like we did for StandardScaler, we instantiate SMOTE within the pipeline\n",
    "pipe = Pipeline(steps = [('smote', SMOTE()), \n",
    "                      ('quantilescaler', QuantileTransformer()),\n",
    "                      ('rfc', KNeighborsClassifier())])\n",
    "pipe.fit(X_train, y_train)\n",
    "# cross validation using intra-fold sampling\n",
    "cross_validate(pipe, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'col':np.random.randn(12000), 'target':np.random.randint(low = 0, high = 2, size=12000)})\n",
    "new_df = df.groupby('target').apply(lambda x: x.sample(n=5000)).reset_index(drop = True)\n",
    "\n",
    "new_df.target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit ('masters_thesis')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ecf9425976ca1d495f6ff342e01edb9b99409cdaa6c55166a5169aaab36ca909"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
